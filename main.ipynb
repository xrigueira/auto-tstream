{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer for time series modeling\n",
    "These are the inputs and outputs to the model.\n",
    "- Inputs: SWIT.\n",
    "- Output: Streamflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import utils\n",
    "import dataset as ds\n",
    "from models.autoformer import Autoformer\n",
    "from models.informer import Informer\n",
    "# from models.fedformer import FEDformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train, test, and validations steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define train step\n",
    "def train(dataloader, model, loss_function, optimizer, device, df_training, epoch):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    training_loss = [] # For plotting purposes\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        src, tgt, tgt_y, src_pe, tgt_pe = batch\n",
    "        src, tgt, tgt_y, src_pe, tgt_pe = src.float().to(device), tgt.float().to(device), tgt_y.float().to(device), src_pe.float().to(device), tgt_pe.float().to(device)\n",
    "        \n",
    "        # Zero out gradients for every batch\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Process decoder input\n",
    "        decoder_input = torch.zeros_like(tgt[:, -output_sequence_len:, :]).float()\n",
    "        decoder_input = torch.cat([tgt[:, :decoder_sequence_len, :], decoder_input], dim=1).float().to(device)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred, attention_weights = model(src, decoder_input, src_pe, tgt_pe)\n",
    "        \n",
    "        f_dim = -1 if features == 'MS' else 0\n",
    "        pred = pred[:, -decoder_sequence_len:, f_dim:].to(device)\n",
    "        tgt_y = tgt_y[:, -decoder_sequence_len:, f_dim:].to(device)\n",
    "        \n",
    "        loss = loss_function(pred, tgt_y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Save results for plotting\n",
    "        training_loss.append(loss.item())\n",
    "        epoch_train_loss = np.mean(training_loss)\n",
    "        df_training.loc[epoch] = [epoch, epoch_train_loss]\n",
    "        \n",
    "        # if i % 20 == 0:\n",
    "        #     print('Current batch', i)\n",
    "        #     loss, current = loss.item(), (i + 1) * len(src)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# Define val step\n",
    "def val(dataloader, model, loss_function, device, df_validation, epoch):\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    validation_loss = [] # For plotting purposes\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src, tgt, tgt_y, src_pe, tgt_pe = batch\n",
    "            src, tgt, tgt_y, src_pe, tgt_pe = src.float().to(device), tgt.float().to(device), tgt_y.float().to(device), src_pe.float().to(device), tgt_pe.float().to(device)\n",
    "            \n",
    "            # Process decoder input\n",
    "            decoder_input = torch.zeros_like(tgt[:, -output_sequence_len:, :]).float()\n",
    "            decoder_input = torch.cat([tgt[:, :decoder_sequence_len, :], decoder_input], dim=1).float().to(device)\n",
    "            \n",
    "            # Compute prediction error\n",
    "            pred, attention_weights = model(src, decoder_input, src_pe, tgt_pe)\n",
    "            \n",
    "            f_dim = -1 if features == 'MS' else 0\n",
    "            pred = pred[:, -decoder_sequence_len:, f_dim:].to(device)\n",
    "            tgt_y = tgt_y[:, -decoder_sequence_len:, f_dim:].to(device)\n",
    "        \n",
    "            loss = loss_function(pred, tgt_y)\n",
    "            \n",
    "            # Save results for plotting\n",
    "            validation_loss.append(loss.item())\n",
    "            epoch_val_loss = np.mean(validation_loss)\n",
    "            df_validation.loc[epoch] = [epoch, epoch_val_loss]\n",
    "    \n",
    "    loss /= num_batches\n",
    "    # print(f\"Avg test loss: {loss:>8f}\")\n",
    "\n",
    "# Define test step\n",
    "def test(dataloader, model):\n",
    "    \n",
    "    # Define lists to store the predictions and ground truth\n",
    "    y_hats = []\n",
    "    tgt_ys = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            src, tgt, tgt_y, src_pe, tgt_pe = batch\n",
    "            src, tgt, tgt_y, src_pe, tgt_pe = src.float().to(device), tgt.float().to(device), tgt_y.float().to(device), src_pe.float().to(device), tgt_pe.float().to(device)\n",
    "            \n",
    "            # Process decoder input\n",
    "            decoder_input = torch.zeros_like(tgt[:, -output_sequence_len:, :]).float()\n",
    "            decoder_input = torch.cat([tgt[:, :decoder_sequence_len, :], decoder_input], dim=1).float().to(device)\n",
    "            \n",
    "            # Compute prediction error\n",
    "            pred, attention_weights = model(src, decoder_input, src_pe, tgt_pe)\n",
    "            \n",
    "            f_dim = -1 if features == 'MS' else 0\n",
    "            pred = pred[:, -decoder_sequence_len:, f_dim:].to(device)\n",
    "            tgt_y = tgt_y[:, -decoder_sequence_len:, f_dim:].to(device)\n",
    "            \n",
    "            y_hat = pred.detach().cpu().numpy()\n",
    "            tgt_y = tgt_y.detach().cpu().numpy()\n",
    "            \n",
    "            y_hats.append(y_hat)\n",
    "            tgt_ys.append(tgt_y)\n",
    "    \n",
    "    y_hats = np.concatenate(y_hats, axis=0)\n",
    "    tgt_ys = np.concatenate(tgt_ys, axis=0)\n",
    "    print('Test shape:', y_hats.shape, tgt_ys.shape)\n",
    "    y_hats = y_hats.reshape(-1, y_hats.shape[-2], y_hats.shape[-1])\n",
    "    tgt_ys = tgt_ys.reshape(-1, tgt_ys.shape[-2], tgt_ys.shape[-1])\n",
    "    print('Test shape:', y_hats.shape, tgt_ys.shape)\n",
    "    \n",
    "    # Get metrics\n",
    "    mae, mse, rmse, mape, mspe = utils.metric(y_hats, tgt_ys)\n",
    "    print('MSE: {}\\nMAE: {}'.format(mse, mae))\n",
    "    \n",
    "    return y_hats.squeeze(), tgt_ys.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Define seed\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    # Hyperparams\n",
    "    batch_size = 128\n",
    "    validation_size = 0.125\n",
    "    src_variables = ['X']\n",
    "    tgt_variables = ['y']\n",
    "    input_variables = src_variables + tgt_variables\n",
    "    timestamp_col_name = \"time\"\n",
    "    model_selection = 'autoformer' # 'autoformer', 'informer', 'fedformer'\n",
    "    \n",
    "    d_model = 16\n",
    "    n_heads = 2\n",
    "    n_encoder_layers = 2\n",
    "    n_decoder_layers = 1\n",
    "    encoder_sequence_len = 96 # length of input given to encoder\n",
    "    decoder_sequence_len = 1 # length of input given to decoder\n",
    "    output_sequence_len = 2 # target sequence length (the informer does not work with 1 step ahead)\n",
    "    encoder_input_size = 1\n",
    "    decoder_input_size = 1\n",
    "    decoder_output_size = 1 \n",
    "    encoder_features_fc_layer = 32\n",
    "    decoder_features_fc_layer = 32\n",
    "    activation = 'gelu'\n",
    "    embed = 'time_frequency'\n",
    "    attention_factor = 1\n",
    "    frequency = 'd'\n",
    "    dropout = 0.05\n",
    "    distill = True # For the Informer: whether to use distilling in encoder, using this argument means not using distilling\n",
    "    output_attention = True # Keep True for now\n",
    "    moving_average = 25\n",
    "    window_size = encoder_sequence_len + output_sequence_len\n",
    "    step_size = 1\n",
    "\n",
    "    # FEDformer specific hyperparams\n",
    "    L = 1\n",
    "    ab = 0\n",
    "    modes = 32\n",
    "    mode_select = 'random'\n",
    "    version = 'Wavelets' # 'Wavelets' or 'Fourier'\n",
    "    base = 'legendre'\n",
    "    cross_activation = 'tanh'\n",
    "    wavelet = 0\n",
    "    \n",
    "    num_workers = 2\n",
    "    features = 'MS' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Get device\n",
    "    device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Read the data\n",
    "    data = utils.read_data(timestamp_col_name=timestamp_col_name)\n",
    "    \n",
    "    # Extract train and test data\n",
    "    training_val_lower_bound = datetime.datetime(1980, 10, 1)\n",
    "    training_val_upper_bound = datetime.datetime(2010, 9, 30)\n",
    "\n",
    "    # Extract train/validation and test data\n",
    "    training_val_data = data[(training_val_lower_bound <= data.time) & (data.time <= training_val_upper_bound)]\n",
    "    testing_data = data[data.time > training_val_upper_bound]\n",
    "\n",
    "    # Extract train/calibrate and test temporal data for positional encoding\n",
    "    training_val_data_pe = training_val_data.iloc[:, :1]\n",
    "    testing_data_pe = testing_data.iloc[:, :1]\n",
    "    \n",
    "    # Adapt to prediction task\n",
    "    if features == 'M' or features == 'MS':\n",
    "        cols_data = data.columns[1:]\n",
    "        training_val_data = training_val_data[cols_data]\n",
    "        testing_data = testing_data[cols_data]\n",
    "    elif features == 'S':\n",
    "        training_val_data = training_val_data[[src_variables[0]]]\n",
    "        testing_data = testing_data[[src_variables[0]]]\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit scaler on the training set\n",
    "    scaler.fit(training_val_data.values)\n",
    "    \n",
    "    training_val_data = scaler.transform(training_val_data.values)\n",
    "    testing_data = scaler.transform(testing_data.values)\n",
    "    \n",
    "    # Make list of (start_idx, end_idx) pairs that are used to slice the time series sequence into chuncks\n",
    "    training_val_indices = utils.get_indices(data=training_val_data, window_size=window_size, step_size=step_size)\n",
    "    \n",
    "    training_indices = training_val_indices[:-(round(len(training_val_indices)*validation_size))]\n",
    "    validation_indices = training_val_indices[(round(len(training_val_indices)*(1-validation_size))):]\n",
    "    \n",
    "    testing_indices = utils.get_indices(data=testing_data, window_size=window_size, step_size=step_size)\n",
    "    \n",
    "    # Extract positional encoding data\n",
    "    training_val_data_pe = utils.positional_encoder(training_val_data_pe, time_encoding='time_frequency', frequency='d')\n",
    "    testing_data_pe = utils.positional_encoder(testing_data_pe, time_encoding='time_frequency', frequency='d')\n",
    "    \n",
    "    # Make instance of the custom dataset class\n",
    "    training_data = ds.AutoTransformerDataset(data=torch.tensor(training_val_data), data_pe=torch.tensor(training_val_data_pe),\n",
    "                                            indices=training_indices, encoder_sequence_len=encoder_sequence_len, \n",
    "                                            decoder_sequence_len=decoder_sequence_len, tgt_sequence_len=output_sequence_len)\n",
    "    validation_data = ds.AutoTransformerDataset(data=torch.tensor(training_val_data), data_pe=torch.tensor(training_val_data_pe),\n",
    "                                            indices=validation_indices, encoder_sequence_len=encoder_sequence_len,\n",
    "                                            decoder_sequence_len=decoder_sequence_len, tgt_sequence_len=output_sequence_len)\n",
    "    testing_data = ds.AutoTransformerDataset(data=torch.tensor(testing_data), data_pe=torch.tensor(testing_data_pe),\n",
    "                                            indices=testing_indices, encoder_sequence_len=encoder_sequence_len,\n",
    "                                            decoder_sequence_len=decoder_sequence_len, tgt_sequence_len=output_sequence_len)\n",
    "    \n",
    "    # Set up the dataloaders\n",
    "    training_val_data = training_data + validation_data # For testing puporses\n",
    "    training_data = DataLoader(training_data, batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "    validation_data = DataLoader(validation_data, batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "    testing_data = DataLoader(testing_data, batch_size=1, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "    training_val_data = DataLoader(training_val_data, batch_size=1, shuffle=False, num_workers=num_workers, drop_last=False) # For testing puporses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Build model\n",
    "    if model_selection == 'autoformer':\n",
    "        model = Autoformer(encoder_sequence_len=encoder_sequence_len, decoder_sequence_len=decoder_sequence_len, output_sequence_len=output_sequence_len,\n",
    "                    encoder_input_size=encoder_input_size, decoder_input_size=decoder_input_size, decoder_output_size=decoder_output_size, \n",
    "                    encoder_features_fc_layer=encoder_features_fc_layer, decoder_features_fc_layer=decoder_features_fc_layer, n_encoder_layers=n_encoder_layers,\n",
    "                    n_decoder_layers=n_decoder_layers, activation=activation, embed=embed, d_model=d_model, n_heads=n_heads, attention_factor=attention_factor,\n",
    "                    frequency=frequency, dropout=dropout, output_attention=output_attention, moving_average=moving_average).float()\n",
    "    elif model_selection == 'informer':\n",
    "        model = Informer(output_sequence_len=output_sequence_len, encoder_input_size=encoder_input_size, decoder_input_size=decoder_input_size, \n",
    "                        decoder_output_size=decoder_output_size, encoder_features_fc_layer=encoder_features_fc_layer, decoder_features_fc_layer=decoder_features_fc_layer, \n",
    "                        n_encoder_layers=n_encoder_layers, n_decoder_layers=n_decoder_layers, activation=activation, embed=embed, d_model=d_model, n_heads=n_heads, \n",
    "                        attention_factor=attention_factor, frequency=frequency, dropout=dropout, distill=distill, output_attention=output_attention).float()\n",
    "    elif model_selection == 'fedformer':\n",
    "        model = FEDformer(encoder_sequence_len=encoder_sequence_len, decoder_sequence_len=decoder_sequence_len, output_sequence_len=output_sequence_len,\n",
    "                        encoder_input_size=encoder_input_size, decoder_input_size=decoder_input_size , decoder_output_size=decoder_output_size, \n",
    "                        encoder_features_fc_layer=encoder_features_fc_layer, decoder_features_fc_layer=decoder_features_fc_layer, n_encoder_layers=n_encoder_layers, \n",
    "                        n_decoder_layers=n_decoder_layers, activation=activation, embed=embed, d_model=d_model, n_heads=n_heads, frequency=frequency, \n",
    "                        dropout=dropout, output_attention=output_attention, moving_average=moving_average, version=version, mode_select=model_selection, \n",
    "                        modes=modes, L=L, base=base, cross_activation=cross_activation, wavelet=wavelet).float()\n",
    "    else:\n",
    "        raise ValueError('Model not implemented')\n",
    "\n",
    "    # Send model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Print model and number of parameters\n",
    "    print('Defined model:\\n', model)\n",
    "    utils.count_parameters(model)\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Update model in the training process and test it\n",
    "    epochs = 5 # 250\n",
    "    start_time = time.time()\n",
    "    df_training = pd.DataFrame(columns=('epoch', 'loss_train'))\n",
    "    df_validation = pd.DataFrame(columns=('epoch', 'loss_test'))\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(training_data, model, loss_function, optimizer, device, df_training, epoch=t)\n",
    "        val(validation_data, model, loss_function, device, df_validation, epoch=t)\n",
    "    print(\"Done! ---Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Inference\n",
    "    y_hats_train_val, tgt_ys_train_val = test(training_val_data, model)\n",
    "    y_hats_test, tgt_ys_test = test(testing_data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Plot loss\n",
    "    plt.figure(1);plt.clf()\n",
    "    plt.plot(df_training['epoch'], df_training['loss_train'], '-o', label='loss train')\n",
    "    plt.plot(df_training['epoch'], df_validation['loss_test'], '-o', label='loss test')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(r'epoch')\n",
    "    plt.ylabel(r'loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot testing results\n",
    "    plt.figure(2);plt.clf()\n",
    "    plt.plot(tgt_ys_train_val, label='observed')\n",
    "    plt.plot(y_hats_train_val, label='predicted')\n",
    "    plt.title('Training and validation results')\n",
    "    plt.xlabel(r'time (days)')\n",
    "    plt.ylabel(r'y')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(2);plt.clf()\n",
    "    plt.plot(tgt_ys_test, label='observed')\n",
    "    plt.plot(y_hats_test, label='predicted')\n",
    "    plt.title('Testing results')\n",
    "    plt.xlabel(r'time (days)')\n",
    "    plt.ylabel(r'y')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NSE metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Metrics\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    nse_train_val = utils.nash_sutcliffe_efficiency(tgt_ys_train_val, y_hats_train_val)\n",
    "    rmse_train_val = np.sqrt(mean_squared_error(tgt_ys_train_val, y_hats_train_val))\n",
    "    pbias_train_val = utils.pbias(tgt_ys_train_val, y_hats_train_val)\n",
    "    kge_train_val = utils.kge(tgt_ys_train_val, y_hats_train_val)\n",
    "    print('\\n-- Train/val results')\n",
    "    print('NSE = ', nse_train_val)\n",
    "    print('RMSE = ', rmse_train_val)\n",
    "    print('PBIAS = ', pbias_train_val)\n",
    "    print('KGE = ', kge_train_val)\n",
    "    \n",
    "    nse_test = utils.nash_sutcliffe_efficiency(tgt_ys_test, y_hats_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(tgt_ys_test, y_hats_test))\n",
    "    pbias_test = utils.pbias(tgt_ys_test, y_hats_test)\n",
    "    kge_test = utils.kge(tgt_ys_test, y_hats_test)\n",
    "    print('\\n-- Testing results')\n",
    "    print('NSE = ', nse_test)\n",
    "    print('RMSE = ', rmse_test)\n",
    "    print('PBIAS = ', pbias_test)\n",
    "    print('KGE = ', kge_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
