{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer for time series modeling\n",
    "These are the inputs and outputs to the model.\n",
    "- Inputs: SWIT.\n",
    "- Output: Streamflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import utils\n",
    "import dataset as ds\n",
    "from models.autoformer import Autoformer\n",
    "from models.informer import Informer\n",
    "from models.fedformer import FEDformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train, validation, test and optimization steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define train step\n",
    "def train(dataloader, model, loss_function, optimizer, device, df_training, epoch):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    training_loss = [] # For plotting purposes\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        src, tgt, tgt_y, src_pe, tgt_pe = batch\n",
    "        src, tgt, tgt_y, src_pe, tgt_pe = src.float().to(device), tgt.float().to(device), tgt_y.float().to(device), src_pe.float().to(device), tgt_pe.float().to(device)\n",
    "        \n",
    "        # Zero out gradients for every batch\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Process decoder input\n",
    "        decoder_input = torch.zeros_like(tgt[:, -output_sequence_len:, :]).float()\n",
    "        decoder_input = torch.cat([tgt[:, :decoder_sequence_len, :], decoder_input], dim=1).float().to(device)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred, attention_weights = model(src, decoder_input, src_pe, tgt_pe)\n",
    "        \n",
    "        f_dim = -1 if features == 'MS' else 0\n",
    "        pred = pred[:, -decoder_sequence_len:, f_dim:].to(device)\n",
    "        tgt_y = tgt_y[:, -decoder_sequence_len:, f_dim:].to(device)\n",
    "        \n",
    "        loss = loss_function(pred, tgt_y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Save results for plotting\n",
    "        training_loss.append(loss.item())\n",
    "        epoch_train_loss = np.mean(training_loss)\n",
    "        df_training.loc[epoch] = [epoch, epoch_train_loss]\n",
    "        \n",
    "        # if i % 20 == 0:\n",
    "        #     print('Current batch', i)\n",
    "        #     loss, current = loss.item(), (i + 1) * len(src)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# Define val step\n",
    "def val(dataloader, model, loss_function, device, df_validation, epoch):\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    validation_loss = [] # For plotting purposes\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src, tgt, tgt_y, src_pe, tgt_pe = batch\n",
    "            src, tgt, tgt_y, src_pe, tgt_pe = src.float().to(device), tgt.float().to(device), tgt_y.float().to(device), src_pe.float().to(device), tgt_pe.float().to(device)\n",
    "            \n",
    "            # Process decoder input\n",
    "            decoder_input = torch.zeros_like(tgt[:, -output_sequence_len:, :]).float()\n",
    "            decoder_input = torch.cat([tgt[:, :decoder_sequence_len, :], decoder_input], dim=1).float().to(device)\n",
    "            \n",
    "            # Compute prediction error\n",
    "            pred, attention_weights = model(src, decoder_input, src_pe, tgt_pe)\n",
    "            \n",
    "            f_dim = -1 if features == 'MS' else 0\n",
    "            pred = pred[:, -decoder_sequence_len:, f_dim:].to(device)\n",
    "            tgt_y = tgt_y[:, -decoder_sequence_len:, f_dim:].to(device)\n",
    "        \n",
    "            loss = loss_function(pred, tgt_y)\n",
    "            \n",
    "            # Save results for plotting\n",
    "            validation_loss.append(loss.item())\n",
    "            epoch_val_loss = np.mean(validation_loss)\n",
    "            df_validation.loc[epoch] = [epoch, epoch_val_loss]\n",
    "    \n",
    "    loss /= num_batches\n",
    "    # print(f\"Avg test loss: {loss:>8f}\")\n",
    "\n",
    "# Define test step\n",
    "def test(dataloader, model):\n",
    "    \n",
    "    # Define lists to store the predictions and ground truth\n",
    "    y_hats = []\n",
    "    tgt_ys = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            src, tgt, tgt_y, src_pe, tgt_pe = batch\n",
    "            src, tgt, tgt_y, src_pe, tgt_pe = src.float().to(device), tgt.float().to(device), tgt_y.float().to(device), src_pe.float().to(device), tgt_pe.float().to(device)\n",
    "            \n",
    "            # Process decoder input\n",
    "            decoder_input = torch.zeros_like(tgt[:, -output_sequence_len:, :]).float()\n",
    "            decoder_input = torch.cat([tgt[:, :decoder_sequence_len, :], decoder_input], dim=1).float().to(device)\n",
    "            \n",
    "            # Compute prediction error\n",
    "            pred, attention_weights = model(src, decoder_input, src_pe, tgt_pe)\n",
    "            \n",
    "            f_dim = -1 if features == 'MS' else 0\n",
    "            pred = pred[:, -decoder_sequence_len:, f_dim:].to(device)\n",
    "            tgt_y = tgt_y[:, -decoder_sequence_len:, f_dim:].to(device)\n",
    "            \n",
    "            y_hat = pred.detach().cpu().numpy()\n",
    "            tgt_y = tgt_y.detach().cpu().numpy()\n",
    "            \n",
    "            y_hats.append(y_hat)\n",
    "            tgt_ys.append(tgt_y)\n",
    "    \n",
    "    y_hats = np.concatenate(y_hats, axis=0)\n",
    "    tgt_ys = np.concatenate(tgt_ys, axis=0)\n",
    "    print('Test shape:', y_hats.shape, tgt_ys.shape)\n",
    "    y_hats = y_hats.reshape(-1, y_hats.shape[-2], y_hats.shape[-1])\n",
    "    tgt_ys = tgt_ys.reshape(-1, tgt_ys.shape[-2], tgt_ys.shape[-1])\n",
    "    print('Test shape:', y_hats.shape, tgt_ys.shape)\n",
    "    \n",
    "    # Get metrics\n",
    "    mae, mse, rmse, mape, mspe = utils.metric(y_hats, tgt_ys)\n",
    "    print('MSE: {}\\nMAE: {}'.format(mse, mae))\n",
    "    \n",
    "    return y_hats.squeeze(), tgt_ys.squeeze()\n",
    "\n",
    "# Define function to train, test and evaluate the model\n",
    "def train_val_test(lr, d_model, n_heads, n_encoder_layers, n_decoder_layers, encoder_features_fc_layer, decoder_features_fc_layer):\n",
    "\n",
    "    # Build model\n",
    "    if model_selection == 'autoformer':\n",
    "        model = Autoformer(encoder_sequence_len=encoder_sequence_len, decoder_sequence_len=decoder_sequence_len, output_sequence_len=output_sequence_len,\n",
    "                    encoder_input_size=encoder_input_size, decoder_input_size=decoder_input_size, decoder_output_size=decoder_output_size, \n",
    "                    encoder_features_fc_layer=encoder_features_fc_layer, decoder_features_fc_layer=decoder_features_fc_layer, n_encoder_layers=n_encoder_layers,\n",
    "                    n_decoder_layers=n_decoder_layers, activation=activation, embed=embed, d_model=d_model, n_heads=n_heads, attention_factor=attention_factor,\n",
    "                    frequency=frequency, dropout=dropout, output_attention=output_attention, moving_average=moving_average).float()\n",
    "    elif model_selection == 'informer':\n",
    "        model = Informer(output_sequence_len=output_sequence_len, encoder_input_size=encoder_input_size, decoder_input_size=decoder_input_size, \n",
    "                        decoder_output_size=decoder_output_size, encoder_features_fc_layer=encoder_features_fc_layer, decoder_features_fc_layer=decoder_features_fc_layer, \n",
    "                        n_encoder_layers=n_encoder_layers, n_decoder_layers=n_decoder_layers, activation=activation, embed=embed, d_model=d_model, n_heads=n_heads, \n",
    "                        attention_factor=attention_factor, frequency=frequency, dropout=dropout, distill=distill, output_attention=output_attention).float()\n",
    "    elif model_selection == 'fedformer':\n",
    "        model = FEDformer(encoder_sequence_len=encoder_sequence_len, decoder_sequence_len=decoder_sequence_len, output_sequence_len=output_sequence_len,\n",
    "                        encoder_input_size=encoder_input_size, decoder_input_size=decoder_input_size , decoder_output_size=decoder_output_size, \n",
    "                        encoder_features_fc_layer=encoder_features_fc_layer, decoder_features_fc_layer=decoder_features_fc_layer, n_encoder_layers=n_encoder_layers, \n",
    "                        n_decoder_layers=n_decoder_layers, activation=activation, embed=embed, d_model=d_model, n_heads=n_heads, frequency=frequency, \n",
    "                        dropout=dropout, output_attention=output_attention, moving_average=moving_average, version=version, mode_select=model_selection, \n",
    "                        modes=modes, L=L, base=base, cross_activation=cross_activation, wavelet=wavelet).float()\n",
    "    else:\n",
    "        raise ValueError('Model not implemented')\n",
    "\n",
    "    # Send model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Update model in the training process and test it\n",
    "    epochs = 5 # 250\n",
    "    start_time = time.time()\n",
    "    df_training = pd.DataFrame(columns=('epoch', 'loss_train'))\n",
    "    df_validation = pd.DataFrame(columns=('epoch', 'loss_test'))\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(training_data, model, loss_function, optimizer, device, df_training, epoch=t)\n",
    "        val(validation_data, model, loss_function, device, df_validation, epoch=t)\n",
    "    print(\"Done! ---Execution time: %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # Inference\n",
    "    # y_hats_train_val, tgt_ys_train_val = test(training_val_data, model)\n",
    "    y_hats_test, tgt_ys_test = test(testing_data, model)\n",
    "\n",
    "    nse_test = utils.nash_sutcliffe_efficiency(tgt_ys_test, y_hats_test)\n",
    "    print('NSE = ', nse_test)\n",
    "\n",
    "    return nse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hypeparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Define seed\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Hyperparams\n",
    "    batch_size = 128\n",
    "    validation_size = 0.125\n",
    "    src_variables = ['X']\n",
    "    tgt_variables = ['y']\n",
    "    input_variables = src_variables + tgt_variables\n",
    "    timestamp_col_name = \"time\"\n",
    "    model_selection = 'autoformer' # 'autoformer', 'informer', 'fedformer'\n",
    "\n",
    "    # d_model = 16\n",
    "    # n_heads = 2\n",
    "    # n_encoder_layers = 2\n",
    "    # n_decoder_layers = 1\n",
    "    encoder_sequence_len = 365 # length of input given to encoder\n",
    "    decoder_sequence_len = 1 # length of input given to decoder\n",
    "    output_sequence_len = 2 # target sequence length (the informer does not work with 1 step ahead)\n",
    "    encoder_input_size = 1\n",
    "    decoder_input_size = 1\n",
    "    decoder_output_size = 1 \n",
    "    # encoder_features_fc_layer = 32\n",
    "    # decoder_features_fc_layer = 32\n",
    "    activation = 'gelu'\n",
    "    embed = 'time_frequency'\n",
    "    attention_factor = 1\n",
    "    frequency = 'd'\n",
    "    dropout = 0.05\n",
    "    distill = True # For the Informer: whether to use distilling in encoder, using this argument means not using distilling\n",
    "    output_attention = True # Keep True for now\n",
    "    moving_average = 25\n",
    "    window_size = encoder_sequence_len + output_sequence_len\n",
    "    step_size = 1\n",
    "\n",
    "    # FEDformer specific hyperparams\n",
    "    L = 1\n",
    "    ab = 0\n",
    "    modes = 32\n",
    "    mode_select = 'random'\n",
    "    version = 'Wavelets' # 'Wavelets' or 'Fourier'\n",
    "    base = 'legendre'\n",
    "    cross_activation = 'tanh'\n",
    "    wavelet = 0\n",
    "\n",
    "    num_workers = 2\n",
    "    features = 'MS' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Get device\n",
    "    device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Read the data\n",
    "    data = utils.read_data(timestamp_col_name=timestamp_col_name)\n",
    "    \n",
    "    # Extract train and test data\n",
    "    training_val_lower_bound = datetime.datetime(1980, 10, 1)\n",
    "    training_val_upper_bound = datetime.datetime(2010, 9, 30)\n",
    "\n",
    "    # Extract train/validation and test data\n",
    "    training_val_data = data[(training_val_lower_bound <= data.time) & (data.time <= training_val_upper_bound)]\n",
    "    testing_data = data[data.time > training_val_upper_bound]\n",
    "\n",
    "    # Extract train/calibrate and test temporal data for positional encoding\n",
    "    training_val_data_pe = training_val_data.iloc[:, :1]\n",
    "    testing_data_pe = testing_data.iloc[:, :1]\n",
    "    \n",
    "    # Adapt to prediction task\n",
    "    if features == 'M' or features == 'MS':\n",
    "        cols_data = data.columns[1:]\n",
    "        training_val_data = training_val_data[cols_data]\n",
    "        testing_data = testing_data[cols_data]\n",
    "    elif features == 'S':\n",
    "        training_val_data = training_val_data[[src_variables[0]]]\n",
    "        testing_data = testing_data[[src_variables[0]]]\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit scaler on the training set\n",
    "    scaler.fit(training_val_data.values)\n",
    "    \n",
    "    training_val_data = scaler.transform(training_val_data.values)\n",
    "    testing_data = scaler.transform(testing_data.values)\n",
    "    \n",
    "    # Make list of (start_idx, end_idx) pairs that are used to slice the time series sequence into chuncks\n",
    "    training_val_indices = utils.get_indices(data=training_val_data, window_size=window_size, step_size=step_size)\n",
    "    \n",
    "    training_indices = training_val_indices[:-(round(len(training_val_indices)*validation_size))]\n",
    "    validation_indices = training_val_indices[(round(len(training_val_indices)*(1-validation_size))):]\n",
    "    \n",
    "    testing_indices = utils.get_indices(data=testing_data, window_size=window_size, step_size=step_size)\n",
    "    \n",
    "    # Extract positional encoding data\n",
    "    training_val_data_pe = utils.positional_encoder(training_val_data_pe, time_encoding='time_frequency', frequency='d')\n",
    "    testing_data_pe = utils.positional_encoder(testing_data_pe, time_encoding='time_frequency', frequency='d')\n",
    "    \n",
    "    # Make instance of the custom dataset class\n",
    "    training_data = ds.AutoTransformerDataset(data=torch.tensor(training_val_data), data_pe=torch.tensor(training_val_data_pe),\n",
    "                                            indices=training_indices, encoder_sequence_len=encoder_sequence_len, \n",
    "                                            decoder_sequence_len=decoder_sequence_len, tgt_sequence_len=output_sequence_len)\n",
    "    validation_data = ds.AutoTransformerDataset(data=torch.tensor(training_val_data), data_pe=torch.tensor(training_val_data_pe),\n",
    "                                            indices=validation_indices, encoder_sequence_len=encoder_sequence_len,\n",
    "                                            decoder_sequence_len=decoder_sequence_len, tgt_sequence_len=output_sequence_len)\n",
    "    testing_data = ds.AutoTransformerDataset(data=torch.tensor(testing_data), data_pe=torch.tensor(testing_data_pe),\n",
    "                                            indices=testing_indices, encoder_sequence_len=encoder_sequence_len,\n",
    "                                            decoder_sequence_len=decoder_sequence_len, tgt_sequence_len=output_sequence_len)\n",
    "    \n",
    "    # Set up the dataloaders\n",
    "    training_val_data = training_data + validation_data # For testing puporses\n",
    "    training_data = DataLoader(training_data, batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "    validation_data = DataLoader(validation_data, batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "    testing_data = DataLoader(testing_data, batch_size=1, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "    training_val_data = DataLoader(training_val_data, batch_size=1, shuffle=False, num_workers=num_workers, drop_last=False) # For testing puporses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up and perform optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Set up experiment hyperparams\n",
    "    experiment_hyperparams = {\n",
    "        'lr': [1e-5, 0.001],\n",
    "        'd_model': [32, 64, 128, 256, 512],\n",
    "        'n_heads': [2, 4, 8],\n",
    "        'n_encoder_layers': [1, 2, 4],\n",
    "        'n_decoder_layers': [1, 2, 4],\n",
    "        'encoder_features_fc_layer': [32, 64, 128],\n",
    "        'decoder_features_fc_layer': [32, 64, 128],\n",
    "    }\n",
    "\n",
    "    # Run optimization loop\n",
    "    counter = 0\n",
    "    optimization_df = pd.DataFrame(columns=('lr', 'd_model', 'n_heads', 'n_encoder_layers', 'n_decoder_layers', 'encoder_features_fc_layer', 'decoder_features_fc_layer', 'nse'))\n",
    "    for lr in experiment_hyperparams['lr']:\n",
    "        for d_model in experiment_hyperparams['d_model']:\n",
    "            for n_heads in experiment_hyperparams['n_heads']:\n",
    "                for n_encoder_layers in experiment_hyperparams['n_encoder_layers']:\n",
    "                    for n_decoder_layers in experiment_hyperparams['n_decoder_layers']:\n",
    "                        for encoder_features_fc_layer in experiment_hyperparams['encoder_features_fc_layer']:\n",
    "                            for decoder_features_fc_layer in experiment_hyperparams['decoder_features_fc_layer']:\n",
    "                                \n",
    "                                print(f'Iteration {counter}')\n",
    "                                nse = train_val_test(lr, d_model, n_heads, n_encoder_layers, n_decoder_layers, encoder_features_fc_layer, decoder_features_fc_layer)\n",
    "                                \n",
    "                                # Append parameters and nse to dataframe\n",
    "                                optimization_df.loc[counter] = [lr, d_model, n_heads, n_encoder_layers, n_decoder_layers, \n",
    "                                                    encoder_features_fc_layer, decoder_features_fc_layer, nse]\n",
    "                                \n",
    "                                # Update counter\n",
    "                                counter += 1\n",
    "                                \n",
    "    # Save dataframe to csv\n",
    "    optimization_df.to_csv('results/optimization_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
